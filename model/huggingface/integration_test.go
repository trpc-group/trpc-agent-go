//
// Tencent is pleased to support the open source community by making trpc-agent-go available.
//
// Copyright (C) 2025 Tencent.  All rights reserved.
//
// trpc-agent-go is licensed under the Apache License Version 2.0.
//
//

package huggingface

import (
	"context"
	"os"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"trpc.group/trpc-go/trpc-agent-go/model"
)

// TestIntegration_RealAPI_NonStreaming æµ‹è¯•çœŸå®çš„ HuggingFace APIï¼ˆéæµå¼ï¼‰
// è¿è¡Œæ­¤æµ‹è¯•éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
//   export HUGGINGFACE_API_KEY=your_api_key
//   export RUN_INTEGRATION_TESTS=true
//   go test -v -run TestIntegration_RealAPI
func TestIntegration_RealAPI_NonStreaming(t *testing.T) {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨é›†æˆæµ‹è¯•
	if os.Getenv("RUN_INTEGRATION_TESTS") != "true" {
		t.Skip("Skipping integration test. Set RUN_INTEGRATION_TESTS=true to run.")
	}

	// ä»ç¯å¢ƒå˜é‡è·å– API Key
	apiKey := os.Getenv("HUGGINGFACE_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping integration test. Set HUGGINGFACE_API_KEY environment variable.")
	}

	t.Log("Running real HuggingFace API integration test (non-streaming)...")

	// ä½¿ç”¨ä¸€ä¸ªå°å‹ã€å¿«é€Ÿå“åº”çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
	// microsoft/DialoGPT-small æ˜¯ä¸€ä¸ªè½»é‡çº§å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆæµ‹è¯•
	modelName := "microsoft/DialoGPT-small"
	
	// å¦‚æœç¯å¢ƒå˜é‡æŒ‡å®šäº†å…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
	if customModel := os.Getenv("HUGGINGFACE_TEST_MODEL"); customModel != "" {
		modelName = customModel
	}

	// åˆ›å»ºæ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(apiKey),
	)
	require.NoError(t, err)
	require.NotNil(t, m)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	intPtr := func(i int) *int { return &i }
	float64Ptr := func(f float64) *float64 { return &f }

	// åˆ›å»ºè¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "Hello! How are you?"},
		},
		GenerationConfig: model.GenerationConfig{
			Stream:      false,
			MaxTokens:   intPtr(50),  // é™åˆ¶ token æ•°é‡ä»¥åŠ å¿«å“åº”
			Temperature: float64Ptr(0.7),
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	t.Log("Sending request to HuggingFace API...")
	responseChan, err := m.GenerateContent(ctx, request)
	require.NoError(t, err)
	require.NotNil(t, responseChan)

	// æ”¶é›†å“åº”
	var responses []*model.Response
	for response := range responseChan {
		responses = append(responses, response)
		
		// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
		if response.Error != nil {
			t.Logf("Response error: %v", response.Error)
		}
	}

	// éªŒè¯å“åº”
	require.NotEmpty(t, responses, "Should receive at least one response")
	
	lastResp := responses[len(responses)-1]
	
	// å¦‚æœæœ‰é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ä½†ä¸å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹ä¸å¯ç”¨ï¼‰
	if lastResp.Error != nil {
		t.Logf("API returned error (this may be expected if model is not available): %v", lastResp.Error)
		t.Logf("Error details: %+v", lastResp.Error)
		// ä¸æ ‡è®°ä¸ºå¤±è´¥ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ¨¡å‹æš‚æ—¶ä¸å¯ç”¨
		return
	}

	// éªŒè¯æˆåŠŸå“åº”
	assert.NotNil(t, lastResp)
	require.NotEmpty(t, lastResp.Choices, "Should have at least one choice")
	
	choice := lastResp.Choices[0]
	assert.NotEmpty(t, choice.Message.Content, "Response content should not be empty")
	assert.Equal(t, model.RoleAssistant, choice.Message.Role, "Response role should be assistant")
	
	t.Logf("âœ… Received response from real API:")
	t.Logf("   Model: %s", modelName)
	t.Logf("   Content: %s", choice.Message.Content)
	
	// éªŒè¯ Usage ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
	if lastResp.Usage != nil {
		t.Logf("   Token usage - Prompt: %d, Completion: %d, Total: %d",
			lastResp.Usage.PromptTokens,
			lastResp.Usage.CompletionTokens,
			lastResp.Usage.TotalTokens)
		assert.Greater(t, lastResp.Usage.TotalTokens, 0, "Total tokens should be greater than 0")
	}
}

// TestIntegration_RealAPI_Streaming æµ‹è¯•çœŸå®çš„ HuggingFace APIï¼ˆæµå¼ï¼‰
// è¿è¡Œæ­¤æµ‹è¯•éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
//   export HUGGINGFACE_API_KEY=your_api_key
//   export RUN_INTEGRATION_TESTS=true
//   go test -v -run TestIntegration_RealAPI
func TestIntegration_RealAPI_Streaming(t *testing.T) {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨é›†æˆæµ‹è¯•
	if os.Getenv("RUN_INTEGRATION_TESTS") != "true" {
		t.Skip("Skipping integration test. Set RUN_INTEGRATION_TESTS=true to run.")
	}

	// ä»ç¯å¢ƒå˜é‡è·å– API Key
	apiKey := os.Getenv("HUGGINGFACE_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping integration test. Set HUGGINGFACE_API_KEY environment variable.")
	}

	t.Log("Running real HuggingFace API integration test (streaming)...")

	// ä½¿ç”¨æ”¯æŒæµå¼å“åº”çš„æ¨¡å‹
	modelName := "microsoft/DialoGPT-small"
	
	if customModel := os.Getenv("HUGGINGFACE_TEST_MODEL"); customModel != "" {
		modelName = customModel
	}

	// åˆ›å»ºæ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(apiKey),
	)
	require.NoError(t, err)
	require.NotNil(t, m)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	intPtr := func(i int) *int { return &i }
	float64Ptr := func(f float64) *float64 { return &f }

	// åˆ›å»ºæµå¼è¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "Tell me a short joke."},
		},
		GenerationConfig: model.GenerationConfig{
			Stream:      true,
			MaxTokens:   intPtr(100),
			Temperature: float64Ptr(0.8),
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	t.Log("Sending streaming request to HuggingFace API...")
	responseChan, err := m.GenerateContent(ctx, request)
	require.NoError(t, err)
	require.NotNil(t, responseChan)

	// æ”¶é›†æ‰€æœ‰æµå¼å“åº”
	var responses []*model.Response
	var fullContent string
	chunkCount := 0

	for response := range responseChan {
		responses = append(responses, response)
		chunkCount++
		
		// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
		if response.Error != nil {
			t.Logf("Chunk %d error: %v", chunkCount, response.Error)
			continue
		}
		
		// ç´¯ç§¯å†…å®¹
		if len(response.Choices) > 0 {
			content := response.Choices[0].Delta.Content
			if content != "" {
				fullContent += content
				t.Logf("Chunk %d: %q", chunkCount, content)
			}
		}
	}

	// éªŒè¯å“åº”
	require.NotEmpty(t, responses, "Should receive at least one response")
	
	lastResp := responses[len(responses)-1]
	
	// å¦‚æœæœ‰é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ä½†ä¸å¤±è´¥
	if lastResp.Error != nil {
		t.Logf("API returned error (this may be expected if model is not available): %v", lastResp.Error)
		return
	}

	// éªŒè¯æµå¼å“åº”
	assert.Greater(t, chunkCount, 0, "Should receive at least one chunk")
	
	t.Logf("âœ… Received streaming response from real API:")
	t.Logf("   Model: %s", modelName)
	t.Logf("   Total chunks: %d", chunkCount)
	t.Logf("   Full content: %s", fullContent)
	
	// éªŒè¯è‡³å°‘æ”¶åˆ°äº†ä¸€äº›å†…å®¹
	if fullContent != "" {
		assert.NotEmpty(t, fullContent, "Should receive some content from streaming")
	}
}

// TestIntegration_RealAPI_WithCallbacks æµ‹è¯•çœŸå® API çš„å›è°ƒæœºåˆ¶
func TestIntegration_RealAPI_WithCallbacks(t *testing.T) {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨é›†æˆæµ‹è¯•
	if os.Getenv("RUN_INTEGRATION_TESTS") != "true" {
		t.Skip("Skipping integration test. Set RUN_INTEGRATION_TESTS=true to run.")
	}

	apiKey := os.Getenv("HUGGINGFACE_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping integration test. Set HUGGINGFACE_API_KEY environment variable.")
	}

	t.Log("Running real HuggingFace API integration test (with callbacks)...")

	// å›è°ƒè®¡æ•°å™¨
	var requestCallbackCalled bool
	var chunkCallbackCount int
	var streamCompleteCallbackCalled bool

	modelName := "microsoft/DialoGPT-small"
	if customModel := os.Getenv("HUGGINGFACE_TEST_MODEL"); customModel != "" {
		modelName = customModel
	}

	// åˆ›å»ºå¸¦å›è°ƒçš„æ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(apiKey),
		WithChatRequestCallback(func(ctx context.Context, req *ChatCompletionRequest) {
			requestCallbackCalled = true
			t.Logf("ğŸ“¤ Request callback: sending request with %d messages", len(req.Messages))
		}),
		WithChatChunkCallback(func(ctx context.Context, req *ChatCompletionRequest, chunk *ChatCompletionChunk) {
			chunkCallbackCount++
			if len(chunk.Choices) > 0 && chunk.Choices[0].Delta.Content != "" {
				t.Logf("ğŸ“¥ Chunk callback #%d: %q", chunkCallbackCount, chunk.Choices[0].Delta.Content)
			}
		}),
		WithChatStreamCompleteCallback(func(ctx context.Context, req *ChatCompletionRequest, streamErr error) {
			streamCompleteCallbackCalled = true
			if streamErr != nil {
				t.Logf("âœ… Stream complete callback: completed with error: %v", streamErr)
			} else {
				t.Logf("âœ… Stream complete callback: completed successfully")
			}
		}),
	)
	require.NoError(t, err)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	intPtr := func(i int) *int { return &i }

	// åˆ›å»ºæµå¼è¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "Say hello!"},
		},
		GenerationConfig: model.GenerationConfig{
			Stream:    true,
			MaxTokens: intPtr(30),
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	responseChan, err := m.GenerateContent(ctx, request)
	require.NoError(t, err)

	// æ¶ˆè´¹æ‰€æœ‰å“åº”
	for response := range responseChan {
		if response.Error != nil {
			t.Logf("Response error: %v", response.Error)
		}
	}

	// ç­‰å¾…å›è°ƒå®Œæˆ
	time.Sleep(100 * time.Millisecond)

	// éªŒè¯å›è°ƒè¢«è°ƒç”¨
	t.Logf("\nğŸ“Š Callback Statistics:")
	t.Logf("   Request callback called: %v", requestCallbackCalled)
	t.Logf("   Chunk callbacks count: %d", chunkCallbackCount)
	t.Logf("   Stream complete callback called: %v", streamCompleteCallbackCalled)

	assert.True(t, requestCallbackCalled, "Request callback should be called")
	assert.True(t, streamCompleteCallbackCalled, "Stream complete callback should be called")
	// Chunk callback å¯èƒ½ä¸ä¼šè¢«è°ƒç”¨ï¼ˆå¦‚æœæ¨¡å‹ä¸å¯ç”¨æˆ–è¿”å›é”™è¯¯ï¼‰
}
