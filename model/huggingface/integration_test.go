//
// Tencent is pleased to support the open source community by making trpc-agent-go available.
//
// Copyright (C) 2025 Tencent.  All rights reserved.
//
// trpc-agent-go is licensed under the Apache License Version 2.0.
//
//

package huggingface

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"trpc.group/trpc-go/trpc-agent-go/model"
)

const modelName = "meta-llama/Llama-3.1-8B-Instruct:ovhcloud"

//const modelName = "Tongyi-MAI/Z-Image-Turbo"

// todo chat modelå’Œéchat modelçš„æ”¯æŒï¼Œéƒ½éœ€è¦
// TestIntegration_RealAPI_NonStreaming æµ‹è¯•çœŸå®çš„ HuggingFace APIï¼ˆéæµå¼ï¼‰
func TestIntegration_RealAPI_NonStreaming(t *testing.T) {
	t.Log("Running real HuggingFace API integration test (non-streaming)...")
	// åˆ›å»ºæ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(ApiKey),
		WithEnableTokenTailoring(true),
		//WithTailoringStrategy(customStrategy),
		WithTokenTailoringConfig(&model.TokenTailoringConfig{
			ProtocolOverheadTokens: 256,
			ReserveOutputTokens:    1024,
			SafetyMarginRatio:      0.05,
		}),
	)
	require.NoError(t, err)
	//require.NotNil(t, m)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	//intPtr := func(i int) *int { return &i }
	//float64Ptr := func(f float64) *float64 { return &f }

	// åˆ›å»ºè¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "ä¸€ä¸ªäººåº”è¯¥æœ‰æ€æ ·çš„ç‰¹åˆ«çš„æ¢¦æƒ³ï¼Ÿ"},
		},
		GenerationConfig: model.GenerationConfig{
			Stream: false,
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	t.Log("Sending request to HuggingFace API...")
	responseChan, err := m.GenerateContent(ctx, request)
	//require.NoError(t, err)
	//require.NotNil(t, responseChan)

	// æ”¶é›†å“åº”
	var responses []*model.Response
	for response := range responseChan {
		responses = append(responses, response)

		// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
		if response.Error != nil {
			t.Logf("Response error: %v", response.Error)
		}
	}

	// éªŒè¯å“åº”
	require.NotEmpty(t, responses, "Should receive at least one response")

	lastResp := responses[len(responses)-1]

	// å¦‚æœæœ‰é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ä½†ä¸å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹ä¸å¯ç”¨ï¼‰
	if lastResp.Error != nil {
		t.Logf("API returned error (this may be expected if model is not available): %v", lastResp.Error)
		t.Logf("Error details: %+v", lastResp.Error)
		// ä¸æ ‡è®°ä¸ºå¤±è´¥ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ¨¡å‹æš‚æ—¶ä¸å¯ç”¨
		return
	}

	// éªŒè¯æˆåŠŸå“åº”
	assert.NotNil(t, lastResp)
	require.NotEmpty(t, lastResp.Choices, "Should have at least one choice")

	choice := lastResp.Choices[0]
	assert.NotEmpty(t, choice.Message.Content, "Response content should not be empty")
	assert.Equal(t, model.RoleAssistant, choice.Message.Role, "Response role should be assistant")

	t.Logf("âœ… Received response from real API:")
	t.Logf("   Model: %s", modelName)
	t.Logf("   Content: %s", choice.Message.Content)

	// éªŒè¯ Usage ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
	if lastResp.Usage != nil {
		t.Logf("   Token usage - Prompt: %d, Completion: %d, Total: %d",
			lastResp.Usage.PromptTokens,
			lastResp.Usage.CompletionTokens,
			lastResp.Usage.TotalTokens)
		assert.Greater(t, lastResp.Usage.TotalTokens, 0, "Total tokens should be greater than 0")
	}
}

// TestIntegration_RealAPI_Streaming æµ‹è¯•çœŸå®çš„ HuggingFace APIï¼ˆæµå¼ï¼‰
func TestIntegration_RealAPI_Streaming(t *testing.T) {
	t.Log("Running real HuggingFace API integration test (streaming)...")
	// åˆ›å»ºæ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(ApiKey),
	)
	require.NoError(t, err)
	require.NotNil(t, m)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	//intPtr := func(i int) *int { return &i }
	//float64Ptr := func(f float64) *float64 { return &f }

	// åˆ›å»ºæµå¼è¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "ç»™æˆ‘è®²ä¸€ä¸ªçŸ­ç¬‘è¯."},
		},
		GenerationConfig: model.GenerationConfig{
			Stream: true,
			//MaxTokens:   intPtr(100),
			//Temperature: float64Ptr(0.8),
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	t.Log("Sending streaming request to HuggingFace API...")
	responseChan, err := m.GenerateContent(ctx, request)
	require.NoError(t, err)
	require.NotNil(t, responseChan)

	// æ”¶é›†æ‰€æœ‰æµå¼å“åº”
	var responses []*model.Response
	var fullContent string
	chunkCount := 0

	for response := range responseChan {
		responses = append(responses, response)
		chunkCount++

		// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
		if response.Error != nil {
			t.Logf("Chunk %d error: %v", chunkCount, response.Error)
			continue
		}

		// ç´¯ç§¯å†…å®¹
		if len(response.Choices) > 0 {
			content := response.Choices[0].Delta.Content
			if content != "" {
				fullContent += content
				//t.Logf("Chunk %d: %q", chunkCount, content)
			}
		}
	}

	// éªŒè¯å“åº”
	require.NotEmpty(t, responses, "Should receive at least one response")

	lastResp := responses[len(responses)-1]

	// å¦‚æœæœ‰é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ä½†ä¸å¤±è´¥
	if lastResp.Error != nil {
		t.Logf("API returned error (this may be expected if model is not available): %v", lastResp.Error)
		return
	}

	// éªŒè¯æµå¼å“åº”
	assert.Greater(t, chunkCount, 0, "Should receive at least one chunk")

	t.Logf("âœ… Received streaming response from real API:")
	t.Logf("   Model: %s", modelName)
	t.Logf("   Total chunks: %d", chunkCount)
	t.Logf("   Full content: %s", fullContent)

	// éªŒè¯è‡³å°‘æ”¶åˆ°äº†ä¸€äº›å†…å®¹
	if fullContent != "" {
		assert.NotEmpty(t, fullContent, "Should receive some content from streaming")
	}
}

// TestIntegration_RealAPI_WithCallbacks æµ‹è¯•çœŸå® API çš„å›è°ƒæœºåˆ¶
func TestIntegration_RealAPI_WithCallbacks(t *testing.T) {

	// å›è°ƒè®¡æ•°å™¨
	var requestCallbackCalled bool
	var chunkCallbackCount int
	var streamCompleteCallbackCalled bool

	// åˆ›å»ºå¸¦å›è°ƒçš„æ¨¡å‹å®ä¾‹
	m, err := New(
		modelName,
		WithAPIKey(ApiKey),
		WithChatRequestCallback(func(ctx context.Context, req *ChatCompletionRequest) {
			requestCallbackCalled = true
			t.Logf("ğŸ“¤ Request callback: sending request with %d messages", len(req.Messages))
		}),
		WithChatChunkCallback(func(ctx context.Context, req *ChatCompletionRequest, chunk *ChatCompletionChunk) {
			chunkCallbackCount++
			if len(chunk.Choices) > 0 && chunk.Choices[0].Delta.Content != "" {
				t.Logf("ğŸ“¥ Chunk callback #%d: %q", chunkCallbackCount, chunk.Choices[0].Delta.Content)
			}
		}),
		WithChatStreamCompleteCallback(func(ctx context.Context, req *ChatCompletionRequest, streamErr error) {
			streamCompleteCallbackCalled = true
			if streamErr != nil {
				t.Logf("âœ… Stream complete callback: completed with error: %v", streamErr)
			} else {
				t.Logf("âœ… Stream complete callback: completed successfully")
			}
		}),
	)
	require.NoError(t, err)

	// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæŒ‡é’ˆ
	//intPtr := func(i int) *int { return &i }

	// åˆ›å»ºæµå¼è¯·æ±‚
	request := &model.Request{
		Messages: []model.Message{
			{Role: model.RoleUser, Content: "ä¸€ä¸ªäººå¯ä»¥æ€æ ·èººå¹³"},
		},
		GenerationConfig: model.GenerationConfig{
			Stream: true,
			//MaxTokens: intPtr(30),
		},
	}

	// æ‰§è¡Œè¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	responseChan, err := m.GenerateContent(ctx, request)
	require.NoError(t, err)

	// æ¶ˆè´¹æ‰€æœ‰å“åº”
	for response := range responseChan {
		if response.Error != nil {
			t.Logf("Response error: %v", response.Error)
		}
	}

	// ç­‰å¾…å›è°ƒå®Œæˆ
	time.Sleep(100 * time.Millisecond)

	// éªŒè¯å›è°ƒè¢«è°ƒç”¨
	t.Logf("\nğŸ“Š Callback Statistics:")
	t.Logf("   Request callback called: %v", requestCallbackCalled)
	t.Logf("   Chunk callbacks count: %d", chunkCallbackCount)
	t.Logf("   Stream complete callback called: %v", streamCompleteCallbackCalled)

	assert.True(t, requestCallbackCalled, "Request callback should be called")
	assert.True(t, streamCompleteCallbackCalled, "Stream complete callback should be called")
	// Chunk callback å¯èƒ½ä¸ä¼šè¢«è°ƒç”¨ï¼ˆå¦‚æœæ¨¡å‹ä¸å¯ç”¨æˆ–è¿”å›é”™è¯¯ï¼‰
}
