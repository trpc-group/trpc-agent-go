# 会话摘要有效性评测

## 1. 引言

大语言模型（LLM）在多轮对话场景中面临上下文窗口限制和 Token 成本问题。会话摘要是一种常见的解决方案：将历史对话压缩为摘要，以减少输入 Token 数量。然而，摘要可能导致关键信息丢失，影响后续回答质量。本文旨在回答以下问题：（1）会话摘要在哪些场景下能有效节省 Token？（2）摘要对响应质量的影响有多大？（3）什么样的摘要触发策略是最优的？

通过在 MT-Bench-101 数据集的 9 个任务（917 个用例）上进行对比实验，我们发现：

- **长对话有效**：≥4 轮对话可实现 28%~40% 的 Prompt Token 节省，同时保持 85% 以上的响应一致性
- **短对话有害**：≤2 轮对话下，摘要机制不仅无法带来收益，反而因摘要生成开销导致 Token 消耗增加
- **触发策略过于激进**：当前设置（每 2 轮触发摘要）对短对话不适用

---

## 2. 方法

### 2.1 实验设计

采用 **A/B 对比实验**设计：

- **基线组（Baseline）**：保留完整对话历史作为上下文
- **实验组（Summary）**：每 N 轮对话后生成摘要，用摘要替代原始历史

对于同一输入，两组使用相同的 LLM 生成回答，比较 Token 消耗和回答质量。

### 2.2 评测指标

参考 τ-bench 和 τ²-bench 方法论，定义三个评测维度：

| 指标           | 权重 | 定义                                                         |
| -------------- | ---: | ------------------------------------------------------------ |
| **响应一致性** |  50% | 摘要组与基线组回答的语义相似度，使用 LLM 评分（0~1）         |
| **Token 效率** |  30% | 节省率 = (Baseline - Summary) / Baseline × 100%              |
| **信息保留率** |  20% | 关键信息（数字、专有名词、引用内容）在摘要后回答中的保留比例 |

**Pass^1 指标**：一致性得分 ≥ 0.7 则视为通过，Pass^1 = 通过用例数 / 总用例数。

### 2.3 数据集

使用 **MT-Bench-101** 数据集，该数据集包含 13 类多轮对话任务。本次评测覆盖 9 个任务：

| 任务代码 | 任务名称                  | 用例数 | 任务描述                           |
| -------- | ------------------------- | -----: | ---------------------------------- |
| CC       | Content Confusion         |    147 | 区分相似但含义不同的查询           |
| CM       | Context Memory            |     80 | 回忆早期对话细节回答当前问题       |
| GR       | General Reasoning         |     71 | 跨轮次协作解决推理问题             |
| IC       | Instruction Clarification |    150 | 对模糊查询进行澄清                 |
| PI       | Proactive Interaction     |     87 | 主动提问引导对话继续               |
| SA       | Self-affirmation          |     73 | 面对不准确反馈坚持正确回答         |
| SC       | Self-correction           |     77 | 根据用户反馈修正回答               |
| SI       | Separate Input            |    149 | 首轮描述任务要求，后续轮次提供输入 |
| TS       | Topic Shift               |     83 | 识别并聚焦用户切换的新话题         |

**未覆盖任务**：AR（指代消解）、CR（内容改写）、FR（格式改写）、MR（数学推理）。

### 2.4 实验配置

| 参数         | 值            | 说明                        |
| ------------ | ------------- | --------------------------- |
| 模型         | deepseek-v3.2 | 用于生成回答和摘要          |
| 摘要触发阈值 | 2             | 每 2 轮对话触发一次摘要     |
| 运行次数     | 1             | 每个用例运行 1 次           |
| 一致性阈值   | 0.7           | Pass^1 的判定阈值           |
| 评估方式     | LLM-eval      | 使用 LLM 进行语义一致性评估 |

---

## 3. 实验结果

### 3.1 总体结果

| 指标                      |            数值 |
| ------------------------- | --------------: |
| 总用例数                  |             917 |
| 总 Baseline Tokens        |       3,515,728 |
| 总 Summary Tokens         |       3,062,518 |
| **总体 Token 节省率**     |      **12.89%** |
| 总 Baseline Prompt Tokens |       1,891,399 |
| 总 Summary Prompt Tokens  |       1,428,606 |
| **总体 Prompt 节省率**    |      **24.47%** |
| 加权平均一致性            |           0.853 |
| 加权 Pass^1               |           92.3% |
| 加权平均保留率            |           0.836 |
| **Token 负节省用例数**    | **329 (35.9%)** |

**关键发现**：虽然总体节省率为正，但超过 1/3 的用例出现了 Token 负节省（即摘要模式消耗了更多 Token）。

### 3.2 分任务结果

**表 1：各任务 Token 效率指标**

| 任务 | 用例数 | Prompt 节省 | Token 节省 |     p25 |    p50 |    p75 |  负节省率 |
| ---- | -----: | ----------: | ---------: | ------: | -----: | -----: | --------: |
| SI   |    149 |      39.50% |     22.59% |   0.88% | 16.67% | 26.47% |     17.4% |
| PI   |     87 |      34.17% |     21.24% |  -2.04% | 12.11% | 23.46% |     26.4% |
| CM   |     80 |      28.07% |     15.83% |   6.93% | 15.42% | 24.08% |     16.2% |
| CC   |    147 |      10.10% |      4.28% |  -7.03% |  1.86% |  9.90% |     42.2% |
| IC   |    150 |       8.89% |      4.97% | -10.45% |  1.20% | 10.98% |     46.0% |
| GR   |     71 |       4.35% |      3.59% |  -9.95% |  0.68% | 10.28% |     43.7% |
| SA   |     73 |       0.95% |      1.54% |  -8.68% |  3.40% | 11.41% |     42.5% |
| TS   |     83 |       0.51% |      0.95% |  -5.86% |  0.95% |  7.78% |     43.4% |
| SC   |     77 |  **-0.50%** | **-1.08%** |  -9.53% |  0.00% |  7.52% | **49.4%** |

**表 2：各任务回答质量指标**

| 任务 |    一致性 |    Pass^1 |    保留率 |
| ---- | --------: | --------: | --------: |
| GR   | **0.916** |     93.0% |     0.870 |
| SC   |     0.881 |     93.5% | **0.872** |
| SA   |     0.862 |     83.6% |     0.865 |
| CC   |     0.861 |     89.1% |     0.860 |
| IC   |     0.851 |     95.3% |     0.825 |
| TS   |     0.846 |     95.2% |     0.849 |
| SI   |     0.841 |     89.3% |     0.857 |
| CM   |     0.819 |     96.2% |     0.817 |
| PI   |     0.814 | **96.6%** |     0.704 |

### 3.3 对话轮数分析

**表 3：各任务对话轮数分布**

| 任务 | 平均轮数 |   2 轮占比 | 3 轮占比 | 4 轮占比 | ≥5 轮占比 |
| ---- | -------: | ---------: | -------: | -------: | --------: |
| SI   |     4.16 |      12.8% |    10.7% |    32.2% |     44.3% |
| PI   |     4.07 |       0.0% |    33.3% |    33.3% |     33.3% |
| CM   |     3.99 |       1.2% |     1.2% |    96.3% |      1.2% |
| GR   |     3.07 |       2.8% |    64.8% |    32.4% |      0.0% |
| TS   |     3.00 |       0.0% |   100.0% |     0.0% |      0.0% |
| IC   |     2.84 |      24.0% |    68.0% |     8.0% |      0.0% |
| CC   |     2.39 |      72.8% |    15.6% |     8.8% |      2.7% |
| SA   | **2.00** | **100.0%** |     0.0% |     0.0% |      0.0% |
| SC   | **2.00** | **100.0%** |     0.0% |     0.0% |      0.0% |

### 3.4 基线 Prompt 长度分析

**表 4：各任务平均 Prompt 长度与节省率关系**

| 任务 | 平均 Baseline Prompt | 平均 Baseline Completion | Prompt 节省率 |
| ---- | -------------------: | -----------------------: | ------------: |
| CM   |                4,404 |                    3,155 |        28.07% |
| SI   |                4,273 |                    2,752 |        39.50% |
| PI   |                2,304 |                    1,456 |        34.17% |
| TS   |                1,912 |                    1,870 |         0.51% |
| IC   |                1,683 |                    1,921 |         8.89% |
| CC   |                1,225 |                    1,571 |        10.10% |
| GR   |                  768 |                      652 |         4.35% |
| SA   |                  395 |                      829 |         0.95% |
| SC   |                  355 |                      702 |        -0.50% |

---

## 4. 分析

### 4.1 摘要有效性的影响因素

#### 4.1.1 对话轮数是决定性因素

实验数据揭示了对话轮数与摘要效果的强相关性：

**正相关任务（效果好）**：

- SI（4.16 轮）、PI（4.07 轮）、CM（3.99 轮）均实现 20%+ 的 Token 节省
- 这些任务的 2 轮对话占比均 < 15%

**负相关任务（效果差）**：

- SA、SC 的 100% 用例仅有 2 轮对话
- 摘要触发阈值为 2，意味着第 2 轮时历史仅 1 条消息，几乎无内容可压缩

**根本原因**：在 `-events 2` 设置下，2 轮对话的摘要时机为：

```
Turn 1: history=[] → 不触发摘要
Turn 2: history=[Turn1] → 触发摘要，但仅 1 条历史，压缩空间极小
```

#### 4.1.2 基线 Prompt 长度决定压缩上限

Prompt 节省率与基线 Prompt 长度呈正相关（Pearson r = 0.72）：

- **高压缩潜力**（>2000 tokens）：SI、CM、PI，节省率 28%~40%
- **低压缩潜力**（<500 tokens）：SA、SC，节省率 ≈ 0%

这符合信息论直觉：输入越长，冗余度越高，压缩空间越大。

#### 4.1.3 摘要开销在短对话中被放大

SC 任务出现 **-1.08% 的负节省**，分析其 Token 分布：

| 指标              | Baseline | Summary | 变化       |
| ----------------- | -------- | ------- | ---------- |
| Prompt Tokens     | 27,341   | 27,477  | +0.50%     |
| Completion Tokens | 54,051   | 54,791  | +1.37%     |
| **Total Tokens**  | 81,392   | 82,268  | **+1.08%** |

摘要生成本身消耗 Token（虽未单独统计），但压缩收益几乎为零，导致净损失。

### 4.2 任务特性对摘要效果的影响

#### 4.2.1 SI（输入分离）为何效果最好？

SI 任务的典型结构：

- **Turn 1**：详细任务说明（通常很长）
- **Turn 2~N**：具体输入（通常较短）

摘要可将冗长的任务说明压缩为关键约束，而具体输入保持完整，因此压缩效率最高。

#### 4.2.2 PI（主动交互）为何保留率最低？

PI 的保留率仅 **0.704**，显著低于其他任务。分析发现：

1. **任务特性**：PI 要求模型"主动提问引导对话"，这类引导性内容在摘要中可能被判定为非核心信息
2. **评估方法局限**：保留率基于关键词匹配，而 PI 的关键信息可能以改写形式存在

但 PI 的 Pass^1 高达 **96.6%**，说明语义层面的一致性良好，关键词匹配可能低估了实际保留效果。

#### 4.2.3 TS（话题切换）为何效果差？

TS 任务要求识别用户的话题切换。当历史被摘要压缩后，话题切换的信号可能被弱化，影响模型判断。这表明：**需要上下文完整性的任务不适合激进摘要**。

### 4.3 实验局限性

#### 4.3.1 未统计摘要生成的 Token 成本

当前评测仅比较 Prompt + Completion Tokens，未计入摘要生成消耗的 Token。实际成本应为：

```
Total Cost = Prompt + Completion + Summary Generation
```

如果计入此成本，负节省用例比例可能更高。

#### 4.3.2 单次运行缺乏统计稳定性

`-num-runs 1` 导致 Pass^k（k > 1）无法有效评估。LLM 输出存在随机性，单次运行的结果可能不稳定。

#### 4.3.3 数据集对话轮数偏短

MT-Bench-101 的平均对话轮数为 2~4 轮，与实际生产环境中的长对话场景存在差距。摘要机制更适合长对话，当前数据集可能低估了其潜力。

---

## 5. 讨论与建议

### 5.1 任务适用性分类

基于实验结果，我们将任务分为三类：

| 适用性         | 特征                           | 任务示例   | 建议                 |
| -------------- | ------------------------------ | ---------- | -------------------- |
| **强烈推荐**   | 平均轮数 ≥4，Prompt >2000      | SI, PI, CM | 启用摘要             |
| **有条件推荐** | 平均轮数 3-4，Prompt 1000~2000 | CC, IC, GR | 根据实际轮数动态决策 |
| **不推荐**     | 平均轮数 ≤2，Prompt <1000      | SA, SC, TS | 禁用摘要             |

### 5.2 后续研究方向

1. **增加摘要 Token 统计**：将摘要生成成本纳入评估体系
2. **长对话数据集验证**：使用对话轮数更多的数据集（如 10+ 轮）验证摘要效果上限
3. **摘要 Prompt 优化**：当前摘要 Prompt 可能过于冗长，可尝试精简以降低开销

---

## 6. 结论与展望

本文通过在 MT-Bench-101 数据集上的实证研究，系统评估了会话摘要机制的有效性。主要结论如下：

1. **长对话场景下摘要有效**：平均 4+ 轮的任务（SI、PI、CM）可实现 28%~40% 的 Prompt 节省，同时保持 85% 以上的响应一致性。

2. **短对话场景下摘要有害**：2 轮对话任务（SA、SC）在当前设置下无法获得收益，反而因摘要开销导致 Token 消耗增加。

3. **触发策略需要优化**：固定 `-events 2` 对短对话过于激进，建议采用动态策略，基于对话轮数或累计 Token 数触发摘要。

4. **评测体系需要完善**：应将摘要生成的 Token 成本纳入总成本计算，以更准确地评估摘要的实际收益。

---

## 附录

### 附录 A：Token 分布详情

| 任务 | Baseline Prompt | Baseline Completion | Summary Prompt | Summary Completion | Prompt Δ | Completion Δ |
| ---- | --------------: | ------------------: | -------------: | -----------------: | -------: | -----------: |
| SI   |         636,677 |             410,062 |        385,205 |            425,101 |  -39.50% |       +3.67% |
| CM   |         352,349 |             252,400 |        253,457 |            255,567 |  -28.07% |       +1.25% |
| PI   |         200,445 |             126,682 |        131,961 |            125,675 |  -34.17% |       -0.79% |
| IC   |         252,440 |             288,191 |        229,989 |            283,796 |   -8.89% |       -1.53% |
| CC   |         180,057 |             230,963 |        161,876 |            231,533 |  -10.10% |       +0.25% |
| TS   |         158,705 |             155,207 |        157,900 |            153,034 |   -0.51% |       -1.40% |
| GR   |          54,541 |              46,263 |         52,171 |             45,011 |   -4.35% |       -2.71% |
| SA   |          28,844 |              60,510 |         28,570 |             59,404 |   -0.95% |       -1.83% |
| SC   |          27,341 |              54,051 |         27,477 |             54,791 |   +0.50% |       +1.37% |

### 附录 B：实验环境

- **评测框架**：trpc-agent-go benchmark/summary
- **模型**：deepseek-v3.2

### 附录 C：指标计算公式

**Token 节省率（总量口径）**：

```
Savings% = (∑Baseline Tokens - ∑Summary Tokens) / ∑Baseline Tokens × 100
```

**一致性得分**：
由 LLM 评估两个回答的语义相似度，输出 0~1 的分数。

**保留率**：

采用规则提取 + 匹配的方法计算：

1. **关键信息提取**（从 Baseline 回答中）：
   - 数字（日期、金额等）：正则 `\b\d+[\d,\.]*\b`
   - 引用内容：正则 `["']([^"']+)["']`
   - 专有名词：正则 `\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b`（排除常见词）
   - 每轮最多提取 10 个关键信息

2. **匹配检测**（在 Summary 回答中）：
   - 精确匹配（不区分大小写）
   - 数字模糊匹配（忽略逗号格式差异）

3. **计算公式**：

```
Retention = 匹配的关键信息数 / 提取的关键信息总数
```

---

## 参考文献

1. Bai, Y., et al. "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues." ACL 2024.
2. Yao, S., et al. "τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains." arXiv:2406.12045, 2024.
3. Chen, W., et al. "τ²-bench: Benchmarking Table-Reasoning Agents." arXiv:2506.07982, 2025.
