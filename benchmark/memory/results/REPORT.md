# Evaluating Long-Term Conversational Memory: An Empirical Study on LoCoMo Benchmark

## 1. Introduction

Long-term conversational memory is a critical capability for AI agents that interact with users across multiple sessions. As conversations accumulate over time, agents must effectively store, retrieve, and reason over past interactions to maintain coherent and personalized responses.

This report evaluates the memory capabilities of trpc-agent-go using the **LoCoMo** (Long-Context Conversational Memory) benchmark. We compare four distinct memory paradigms across two storage backends, analyzing their strengths and weaknesses across different question categories.

**Key findings:**

- **Long-context baseline** achieves the highest overall F1 (0.472), confirming that full context remains the gold standard when feasible.
- **Auto memory extraction** (F1=0.357 pgvector) is the strongest memory-based approach, reaching 75.6% of the long-context baseline.
- **RAG-based memory** (F1=0.325 pgvector) provides strong adversarial robustness (0.955) but struggles with factual recall.
- **Agentic memory** (F1=0.294 pgvector) shows that LLM-driven memory extraction faces information density challenges.
- **pgvector consistently outperforms MySQL** by 2-7% F1 across all scenarios, validating the value of vector similarity search.

---

## 2. Methodology

### 2.1 Benchmark Dataset

We use the **LoCoMo** dataset (Maharana et al., 2024), which contains multi-session conversations between pairs of speakers. Each sample includes:

- 15-25 conversation sessions spanning several months.
- Ground-truth QA pairs across 5 categories.
- Session-level observations and summaries.

**Evaluation scale**: 10 samples, 1,986 total QA pairs.

### 2.2 Evaluation Scenarios

| Scenario | Description | Memory Write | Memory Read |
| --- | --- | --- | --- |
| **Long-Context** | Full transcript as LLM context | N/A (all-in-context) | N/A |
| **RAG** | Pre-computed observations stored as memories | Bulk insert | Top-K retrieval |
| **Agentic** | LLM agent decides what to store via tool calls | LLM tool calls (memory_add) | Top-K retrieval |
| **Auto** | Background extractor auto-generates memories | Async extraction | Top-K retrieval |

### 2.3 Storage Backends

| Backend | Retrieval Method | Embedding Model |
| --- | --- | --- |
| **pgvector** | Vector similarity (cosine) | text-embedding-3-small |
| **MySQL** | Full-text search (BM25-like) | N/A |

### 2.4 Evaluation Metrics

Aligned with the LoCoMo paper and industry standards (Mem0, MemMachine):

| Metric | Description |
| --- | --- |
| **F1 Score** | Token-level F1 (primary metric) |
| **BLEU Score** | N-gram overlap precision |
| **LLM Score** | LLM-as-Judge semantic evaluation (0-1) |

### 2.5 QA Categories

| Category | Count | Description |
| --- | --- | --- |
| single-hop | 282 | Single fact from one conversation segment |
| multi-hop | 321 | Requires combining facts from multiple segments |
| temporal | 96 | Temporal reasoning (when did X happen?) |
| open-domain | 841 | Open-ended questions requiring world knowledge |
| adversarial | 446 | Questions designed to test robustness (unanswerable) |

### 2.6 Experimental Configuration

| Parameter | Value |
| --- | --- |
| Model | gpt-4o-mini |
| Evaluation Model | gpt-4o-mini |
| Top-K | 30 |
| Samples | 10 (full LoCoMo-10) |
| Total Questions | 1,986 |
| LLM Judge | Enabled |

---

## 3. Results

### 3.1 Overall Results

| Scenario | Backend | F1 | BLEU | LLM Score | Avg Latency |
| --- | --- | ---: | ---: | ---: | ---: |
| Long-Context | - | **0.472** | **0.429** | **0.523** | 3,485ms |
| RAG | pgvector | 0.325 | 0.304 | 0.359 | 3,739ms |
| RAG | MySQL | 0.308 | 0.288 | 0.353 | 2,793ms |
| Agentic | pgvector | 0.294 | 0.279 | 0.287 | 4,998ms |
| Agentic | MySQL | 0.286 | 0.271 | 0.285 | 4,392ms |
| Auto | pgvector | 0.357 | 0.333 | 0.366 | 5,622ms |
| Auto | MySQL | 0.347 | 0.320 | 0.362 | 5,678ms |

> **Known Issue**: RAG and Auto scenarios for samples locomo10\_2/5/6/7/8 have **zero observations** due to a parsing bug in `parseObservations()`. See Section 4.5 for details.

### 3.2 Results by Category

**Table 1: F1 Score by Category**

| Category | Long-Context | RAG pgvec | RAG MySQL | Agentic pgvec | Agentic MySQL | Auto pgvec | Auto MySQL |
| --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| single-hop | 0.330 | 0.166 | 0.127 | 0.146 | 0.168 | 0.272 | 0.306 |
| multi-hop | 0.319 | 0.086 | 0.098 | 0.178 | 0.135 | 0.088 | 0.101 |
| temporal | 0.088 | 0.075 | 0.057 | 0.091 | 0.043 | 0.060 | 0.056 |
| open-domain | 0.518 | 0.163 | 0.140 | 0.126 | 0.146 | 0.302 | 0.325 |
| adversarial | 0.668 | **0.955** | **0.944** | 0.830 | 0.787 | 0.771 | 0.653 |

**Table 2: LLM Score by Category**

| Category | Long-Context | RAG pgvec | RAG MySQL | Agentic pgvec | Agentic MySQL | Auto pgvec |
| --- | ---: | ---: | ---: | ---: | ---: | ---: |
| single-hop | 0.333 | 0.163 | 0.158 | 0.122 | 0.130 | 0.220 |
| multi-hop | 0.252 | 0.066 | 0.055 | 0.107 | 0.093 | 0.049 |
| temporal | 0.155 | 0.133 | 0.104 | 0.137 | 0.057 | 0.068 |
| open-domain | 0.654 | 0.245 | 0.247 | 0.141 | 0.171 | 0.355 |
| adversarial | 0.670 | **0.955** | **0.944** | 0.830 | 0.787 | 0.771 |

### 3.3 Per-Sample Results

**Table 3: F1 Score per Sample (Long-Context / RAG pgvector / Auto pgvector)**

| Sample | #QA | Long-Context | RAG pgvec | Auto pgvec | Agentic pgvec |
| --- | ---: | ---: | ---: | ---: | ---: |
| locomo10_1 | 199 | 0.429 | 0.399 | 0.311 | 0.279 |
| locomo10_2 | 105 | 0.510 | 0.229 | 0.322 | 0.345 |
| locomo10_3 | 193 | 0.530 | 0.450 | 0.441 | 0.295 |
| locomo10_4 | 260 | 0.456 | 0.415 | 0.367 | 0.335 |
| locomo10_5 | 242 | 0.447 | 0.266 | 0.364 | 0.297 |
| locomo10_6 | 158 | 0.539 | 0.223 | 0.204 | 0.287 |
| locomo10_7 | 190 | 0.465 | 0.215 | 0.404 | 0.278 |
| locomo10_8 | 239 | 0.461 | 0.202 | 0.339 | 0.268 |
| locomo10_9 | 196 | 0.448 | 0.393 | 0.380 | 0.268 |
| locomo10_10 | 204 | 0.480 | 0.398 | 0.393 | 0.297 |
| **Average** | **199** | **0.472** | **0.325** | **0.357** | **0.294** |

---

## 4. Analysis

### 4.1 Scenario Comparison

```
F1 Score Comparison (10 samples, 1986 QA pairs)

long_context   |==========================================| 0.472
auto_pgvector  |================================          | 0.357
auto_mysql     |===============================           | 0.347
rag_pgvector   |=============================             | 0.325
rag_mysql      |============================              | 0.308
agentic_pgvec  |==========================                | 0.294
agentic_mysql  |=========================                 | 0.286
               +------------------------------------------+
               0.0      0.1      0.2      0.3      0.4   0.5
```

#### 4.1.1 Long-Context is the Gold Standard

Long-context achieves the highest F1 (0.472) across all categories except adversarial. This confirms that when context window permits, providing the full conversation transcript yields the best factual recall. However, this approach does not scale to arbitrarily long conversation histories in production.

#### 4.1.2 Auto Extraction Outperforms RAG and Agentic

Auto memory extraction (F1=0.357) outperforms both RAG (0.325) and agentic (0.294):

- **vs. RAG**: Auto generates higher-density, semantically richer memories than raw observations. Single-hop F1 improves from 0.166 to 0.272 (+64%).
- **vs. Agentic**: Auto extraction is more systematicâ€”it processes all conversation content rather than relying on the LLM agent's selective tool calls.

#### 4.1.3 Adversarial Robustness Inversely Correlates with Recall

RAG achieves the highest adversarial F1 (0.955), while long-context scores only 0.668. This is because memory-based approaches naturally return "information not available" when no relevant memory is retrieved, which is the correct answer for adversarial questions. Long-context, with the full transcript available, more often hallucinates plausible but incorrect answers.

### 4.2 Category-Level Analysis

#### 4.2.1 Temporal Reasoning is Universally Weak

Temporal questions have the lowest F1 across all scenarios (0.043-0.091), including long-context (0.088). This indicates that temporal reasoning is fundamentally hard for gpt-4o-mini, regardless of the memory architecture.

Root causes:
- Conversations use relative time references ("last year", "next month") that require resolution against session dates.
- Even with explicit `[DATE:]` prefixes in stored memories, the model struggles to compute temporal relationships.

#### 4.2.2 Multi-hop Benefits from Agentic Memories

Agentic (pgvector) achieves the highest multi-hop F1 (0.178) among memory-based approaches, surpassing RAG (0.086) and Auto (0.088). This suggests that the agentic approach, while extracting fewer memories overall, creates more interconnected knowledge that aids multi-hop reasoning. The `[DATE:]` prefix injected by the date-aware memory service contributes to this advantage.

#### 4.2.3 Open-Domain Questions Favor Rich Context

Long-context dominates open-domain questions (F1=0.518), while memory-based approaches struggle (0.126-0.302). Open-domain questions often require nuanced understanding of conversational context, preferences, and attitudes that are difficult to capture in discrete memory entries.

### 4.3 Backend Comparison: pgvector vs MySQL

| Scenario | pgvector F1 | MySQL F1 | Delta |
| --- | ---: | ---: | ---: |
| RAG | 0.325 | 0.308 | +0.017 |
| Agentic | 0.294 | 0.286 | +0.008 |
| Auto | 0.357 | 0.347 | +0.010 |

pgvector consistently outperforms MySQL:
- **Semantic matching**: Vector similarity captures paraphrased queries that keyword search misses.
- **Largest gap in RAG**: RAG stores longer observation texts where semantic retrieval provides more benefit.
- **Smallest gap in Agentic**: Agentic memories are shorter and more keyword-rich, narrowing the retrieval gap.

### 4.4 Variance Analysis

Per-sample F1 shows notable variance:
- **Long-context**: 0.429 - 0.539 (range 0.110), relatively stable.
- **RAG pgvector**: 0.202 - 0.450 (range 0.248), high variance.
- **Auto pgvector**: 0.204 - 0.441 (range 0.237), high variance.

The high variance in memory-based approaches suggests that some conversation structures are inherently harder for memory extraction and retrieval. Samples with more complex, interleaved topics (e.g., locomo10_6, locomo10_8) tend to score lower.

### 4.5 Observation Parsing Bug (Critical)

**Impact**: RAG and Auto scenarios for 5 out of 10 samples (locomo10\_2/5/6/7/8) have **zero observations written to memory**, causing all non-adversarial questions to return "The information is not available." (F1=0).

**Root Cause**: The Go type `locomo10Observation` is defined as `map[string]map[string][][]string`, requiring the innermost evidence references to be `string`. However, some LoCoMo-10 entries contain **nested lists** as evidence references (e.g., `["D15:3", "D15:5"]` instead of `"D15:3"`). This causes `json.Unmarshal` to fail silently for the entire `observation` field, and `parseObservations()` returns `nil`.

**Affected samples and evidence**:

| Sample | Issue Location | Actual Type |
| --- | --- | --- |
| locomo10\_2 | session\_15\_observation/Jon\[1\]\[1\] | `["D15:3", "D15:5"]` |
| locomo10\_5 | session\_8\_observation/Tim\[2\]\[1\] | `["D8:24", "D8:26", "D8:28"]` |
| locomo10\_6 | session\_27\_observation/Andrew\[2\]\[1\] | `["D27:7", "D27:9", "D27:15", "D27:17"]` |
| locomo10\_7 | session\_3\_observation/John\[2\]\[1\] | `["D3:9", "D3:11"]` |
| locomo10\_8 | session\_4\_observation/Jolene\[2\]\[1\] | `["D4:21", "D4:23"]` |

**Consequence**: RAG scenario scores are inflated for the 5 working samples and deflated overall. The true RAG F1 (if all 10 samples had observations) would likely be higher. Agentic and Long-Context scenarios are **not affected** because they do not rely on pre-parsed observations.

**Fix**: Change the observation entry type from `[]string` to `[]interface{}` (or `[]json.RawMessage`) and extract only the first element as the observation text, treating subsequent elements as evidence references regardless of their type.

---

## 5. Comparison with External Baselines

| System | Model | F1 | Notes |
| --- | --- | ---: | --- |
| GPT-4 (4K context) | GPT-4 | 0.321 | LoCoMo paper baseline |
| GPT-3.5-16K | GPT-3.5 | 0.378 | LoCoMo paper baseline |
| **trpc-agent-go (Long-Context)** | gpt-4o-mini | **0.472** | This work |
| **trpc-agent-go (Auto pgvector)** | gpt-4o-mini | **0.357** | This work |
| **trpc-agent-go (RAG pgvector)** | gpt-4o-mini | **0.325** | This work |
| RAG-Observation (LoCoMo) | GPT-3.5 | 0.414 | LoCoMo paper |

> Note: Direct comparison is approximate as model versions and configurations differ.

Our long-context result (0.472) significantly outperforms LoCoMo's GPT-4 4K baseline (0.321) due to gpt-4o-mini's larger context window. The Auto pgvector result (0.357) is competitive with GPT-3.5-16K's full-context performance (0.378).

---

## 6. Discussion

### 6.1 Strengths of the Current Implementation

1. **Date-aware memory service**: The `datePrefixMemoryService` wrapper automatically injects `[DATE: ...]` prefixes into agentic memories, improving temporal reasoning without relying on LLM compliance.
2. **Strong adversarial robustness**: Memory-based approaches achieve 0.77-0.96 adversarial F1, correctly identifying unanswerable questions.
3. **Auto extraction**: The background extractor provides a good balance between memory quality and system complexity.

### 6.2 Limitations and Future Work

1. **Temporal reasoning remains weak** (F1 < 0.1 across all scenarios). Future work should explore dedicated temporal indexing and reasoning modules.
2. **Multi-hop reasoning gap**: Memory-based approaches struggle to combine facts across sessions. Graph-based memory structures could help.
3. **Open-domain performance**: Memory compression inevitably loses nuance. Hierarchical memory (summary + detail) may bridge this gap.
4. **Model capability ceiling**: gpt-4o-mini's extraction and reasoning abilities limit all scenarios. Stronger models (e.g., GPT-4o, Claude) may substantially improve results.

### 6.3 Recommendations for Production Use

| Use Case | Recommended Approach |
| --- | --- |
| Short conversation history (< 50K tokens) | Long-context (no memory needed) |
| Long-running agents (months of history) | Auto extraction + pgvector |
| High adversarial robustness required | RAG + pgvector |
| Low latency required | RAG + MySQL |

---

## 7. Conclusion

This evaluation demonstrates that trpc-agent-go's memory system provides effective long-term conversational memory across multiple paradigms. The Auto extraction approach with pgvector backend achieves the best balance of recall and robustness, reaching 75.6% of the long-context baseline's F1 score while maintaining strong adversarial robustness (0.771).

Key takeaways:
1. **Auto > RAG > Agentic** for overall memory effectiveness with gpt-4o-mini.
2. **pgvector > MySQL** for semantic retrieval quality.
3. **Temporal reasoning** is the primary bottleneck across all approaches.
4. **Adversarial robustness** is a natural strength of memory-based systems.

---

## Appendix

### A. Experimental Environment

| Component | Version/Config |
| --- | --- |
| Framework | trpc-agent-go |
| Model | gpt-4o-mini |
| Embedding | text-embedding-3-small |
| PostgreSQL | 15+ with pgvector extension |
| MySQL | 8.0+ with full-text search |
| Top-K | 30 |
| Dataset | LoCoMo-10 (10 samples, 1,986 QA) |

### B. Full Category Breakdown (F1 / BLEU / LLM)

| Scenario | single-hop | multi-hop | temporal | open-domain | adversarial |
| --- | --- | --- | --- | --- | --- |
| Long-Context | 0.330/0.260/0.333 | 0.319/0.285/0.252 | 0.088/0.069/0.155 | 0.518/0.456/0.654 | 0.668/0.667/0.670 |
| RAG pgvec | 0.166/0.132/0.163 | 0.086/0.067/0.066 | 0.075/0.060/0.133 | 0.163/0.134/0.245 | 0.955/0.955/0.955 |
| RAG MySQL | 0.127/0.094/0.158 | 0.098/0.079/0.055 | 0.057/0.043/0.104 | 0.140/0.113/0.247 | 0.944/0.944/0.944 |
| Agentic pgvec | 0.146/0.106/0.122 | 0.178/0.160/0.107 | 0.091/0.075/0.137 | 0.126/0.114/0.141 | 0.830/0.830/0.830 |
| Agentic MySQL | 0.168/0.125/0.130 | 0.135/0.119/0.093 | 0.043/0.034/0.057 | 0.146/0.132/0.171 | 0.787/0.787/0.787 |
| Auto pgvec | 0.272/0.209/0.220 | 0.088/0.081/0.049 | 0.060/0.047/0.068 | 0.302/0.271/0.355 | 0.771/0.771/0.771 |
| Auto MySQL | 0.306/0.232/0.277 | 0.101/0.092/0.064 | 0.056/0.040/0.083 | 0.325/0.293/0.380 | 0.653/0.653/0.653 |

### C. Total Evaluation Time

| Scenario | Backend | Total Time | Avg Latency/QA |
| --- | --- | --- | --- |
| Long-Context | - | 1h55m | 3,485ms |
| RAG | pgvector | 2h04m | 3,739ms |
| RAG | MySQL | 1h32m | 2,793ms |
| Agentic | pgvector | 2h45m | 4,998ms |
| Agentic | MySQL | 2h25m | 4,392ms |
| Auto | pgvector | 3h06m | 5,622ms |
| Auto | MySQL | 3h08m | 5,678ms |

---

## References

1. Maharana, A., Lee, D., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. "LoCoMo: Long-Context Conversational Memory." arXiv:2402.17753, 2024.
2. Hu, C., et al. "Memory in the Age of AI Agents." arXiv:2512.13564, 2024.
