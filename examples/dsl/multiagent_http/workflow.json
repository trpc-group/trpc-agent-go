{
  "version": "1.0",
  "name": "multiagent_http_demo",
  "description": "Classifier routes between HTTP request and chat agent, demonstrating builtin.http_request and structured_output variables.",
  "nodes": [
    {
      "id": "start",
      "label": "Start",
      "node_type": "builtin.start",
      "config": {}
    },
    {
      "id": "classifier",
      "label": "classifier",
      "node_type": "builtin.llmagent",
      "config": {
        "model_name": "deepseek-chat",
        "instruction": "You are a routing assistant. Analyze the user's request and decide if it should be answered via an HTTP echo service or directly by chat.\n\nRouting rules:\n- If the user explicitly asks to call an HTTP endpoint, echo some text via HTTP, or inspect an HTTP response, classify as 'http'.\n- Otherwise classify as 'chat'.\n\nStructured output requirements:\n- Always set classification strictly to one of: 'http' or 'chat'.\n- When classification = 'http':\n  - Extract the SHORT exact text that should be echoed (for example, if the user says \\\"echo the text 'hello-http'\\\", then text_to_echo MUST be exactly \\\"hello-http\\\" without quotes or extra words).\n  - Put this short value into text_to_echo.\n- When classification = 'chat':\n  - Set text_to_echo to an empty string.\n\nOnly use the provided JSON schema fields; do not add extra keys.",
        "description": "Route between HTTP call and chat agent",
        "structured_output": {
          "type": "object",
          "properties": {
            "classification": {
              "type": "string",
              "enum": ["http", "chat"],
              "description": "Routing decision: http or chat"
            },
            "text_to_echo": {
              "type": "string",
              "description": "Short text that should be echoed via HTTP when classification is http (e.g., hello-http)"
            },
            "reasoning": {
              "type": "string",
              "description": "Brief reasoning for the decision"
            }
          },
          "required": ["classification", "text_to_echo", "reasoning"]
        },
        "temperature": 0.1
      }
    },
    {
      "id": "http_request",
      "label": "http_request",
      "node_type": "builtin.http_request",
      "config": {
        "method": "GET",
        "url_template": "https://httpbin.org/get?cls={{nodes.classifier.output_parsed.classification}}&echo={{nodes.classifier.output_parsed.text_to_echo}}"
      }
    },
    {
      "id": "chat_agent",
      "label": "chat_agent",
      "node_type": "builtin.llmagent",
      "config": {
        "model_name": "deepseek-chat",
        "instruction": "You are a helpful chat assistant. Answer the user's question directly without calling external HTTP APIs.",
        "description": "Direct chat agent",
        "stream": true,
        "temperature": 0.7,
        "max_tokens": 800
      }
    }
  ],
  "edges": [
    {
      "id": "edge_start_classifier",
      "source": "start",
      "target": "classifier"
    },
    {
      "id": "edge_http_request_end",
      "source": "http_request",
      "target": "__end__"
    },
    {
      "id": "edge_chat_agent_end",
      "source": "chat_agent",
      "target": "__end__"
    }
  ],
  "conditional_edges": [
    {
      "id": "route_by_classification",
      "from": "classifier",
      "condition": {
        "type": "builtin",
        "cases": [
          {
            "name": "http",
            "predicate": {
              "expression": "input.output_parsed.classification == \"http\"",
              "format": "cel"
            },
            "target": "http_request"
          }
        ],
        "default": "chat_agent"
      }
    }
  ],
  "start_node_id": "start"
}
